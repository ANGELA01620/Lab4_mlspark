{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95492266-b3ef-4845-aecc-b9a67f4d221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, when, isnull\n",
    "\n",
    "# %%\n",
    "# Configurar SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SECOP_FeatureEngineering\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d550b11c-f53b-48ef-8f9f-1fcf222fa970",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/opt/spark-data/processed/secop_eda.parquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cargar datos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/opt/spark-data/processed/secop_eda.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRegistros cargados: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.count()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Explorar columnas disponibles\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/sql/readwriter.py:544\u001b[39m, in \u001b[36mDataFrameReader.parquet\u001b[39m\u001b[34m(self, *paths, **options)\u001b[39m\n\u001b[32m    533\u001b[39m int96RebaseMode = options.get(\u001b[33m\"\u001b[39m\u001b[33mint96RebaseMode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    534\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m    535\u001b[39m     mergeSchema=mergeSchema,\n\u001b[32m    536\u001b[39m     pathGlobFilter=pathGlobFilter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m     int96RebaseMode=int96RebaseMode,\n\u001b[32m    542\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/opt/spark-data/processed/secop_eda.parquet."
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "df = spark.read.parquet(\"/opt/spark-data/processed/secop_eda.parquet\")\n",
    "print(f\"Registros cargados: {df.count():,}\")\n",
    "\n",
    "# %%\n",
    "# Explorar columnas disponibles\n",
    "print(\"Columnas disponibles:\")\n",
    "for col_name in df.columns:\n",
    "    print(f\"  - {col_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34206543-4446-4c66-97ef-6fff8b3440d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar features para el modelo\n",
    "# Variables categóricas\n",
    "categorical_cols = [\"departamento\", \"tipo_de_contrato\", \"estado_contrato\"]\n",
    "\n",
    "# Variables numéricas\n",
    "numeric_cols = [\"plazo_de_ejec_del_contrato\", \"valor_del_contrato_num\"]\n",
    "\n",
    "# Verificar qué columnas existen\n",
    "available_cat = [c for c in categorical_cols if c in df.columns]\n",
    "available_num = [c for c in numeric_cols if c in df.columns]\n",
    "\n",
    "print(f\"Categóricas disponibles: {available_cat}\")\n",
    "print(f\"Numéricas disponibles: {available_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c763b50f-d39b-4377-877b-58942c439ae5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Limpiar datos: eliminar nulos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_clean = \u001b[43mdf\u001b[49m.dropna(subset=available_cat + available_num)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRegistros después de limpiar nulos: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_clean.count()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Limpiar datos: eliminar nulos\n",
    "df_clean = df.dropna(subset=available_cat + available_num)\n",
    "print(f\"Registros después de limpiar nulos: {df_clean.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70c9011c-bfe7-48f4-9e75-4b1d37274340",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'available_cat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Convierte strings a índices numéricos\u001b[39;00m\n\u001b[32m      2\u001b[39m indexers = [\n\u001b[32m      3\u001b[39m     StringIndexer(inputCol=col, outputCol=col + \u001b[33m\"\u001b[39m\u001b[33m_idx\u001b[39m\u001b[33m\"\u001b[39m, handleInvalid=\u001b[33m\"\u001b[39m\u001b[33mkeep\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mavailable_cat\u001b[49m\n\u001b[32m      5\u001b[39m ]\n",
      "\u001b[31mNameError\u001b[39m: name 'available_cat' is not defined"
     ]
    }
   ],
   "source": [
    "# Convierte strings a índices numéricos\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col + \"_idx\", handleInvalid=\"keep\")\n",
    "    for col in available_cat\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef48791c-cb66-4163-b7e0-44eee62ea788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringIndexers creados:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'indexers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStringIndexers creados:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[43mindexers\u001b[49m:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx.getInputCol()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx.getOutputCol()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'indexers' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"StringIndexers creados:\")\n",
    "for idx in indexers:\n",
    "    print(f\"  - {idx.getInputCol()} -> {idx.getOutputCol()}\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77abef71-a9ef-464e-84f6-904338ae8f8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'available_cat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# PASO 2: OneHotEncoder para generar variables dummy\u001b[39;00m\n\u001b[32m      2\u001b[39m encoders = [\n\u001b[32m      3\u001b[39m     OneHotEncoder(inputCol=col + \u001b[33m\"\u001b[39m\u001b[33m_idx\u001b[39m\u001b[33m\"\u001b[39m, outputCol=col + \u001b[33m\"\u001b[39m\u001b[33m_vec\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mavailable_cat\u001b[49m\n\u001b[32m      5\u001b[39m ]\n",
      "\u001b[31mNameError\u001b[39m: name 'available_cat' is not defined"
     ]
    }
   ],
   "source": [
    "# PASO 2: OneHotEncoder para generar variables dummy\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=col + \"_idx\", outputCol=col + \"_vec\")\n",
    "    for col in available_cat\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7467120-8d9a-41bc-8e41-0350d2c9d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nOneHotEncoders creados:\")\n",
    "for enc in encoders:\n",
    "    print(f\"  - {enc.getInputCol()} -> {enc.getOutputCol()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83978b0-f49c-48cb-85bd-49698ed67f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 3: VectorAssembler para combinar todas las features\n",
    "# Combinamos: features numéricas + features categóricas codificadas\n",
    "feature_cols = available_num + [col + \"_vec\" for col in available_cat]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "print(f\"\\nVectorAssembler combinará: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22bd16fb-4da5-4ba9-ad82-27c0b123122f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indexers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# PASO 4: Construir Pipeline\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Pipeline = secuencia de transformaciones\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m pipeline_stages = \u001b[43mindexers\u001b[49m + encoders + [assembler]\n\u001b[32m      5\u001b[39m pipeline = Pipeline(stages=pipeline_stages)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPipeline con \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pipeline_stages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m stages:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'indexers' is not defined"
     ]
    }
   ],
   "source": [
    "# PASO 4: Construir Pipeline\n",
    "# Pipeline = secuencia de transformaciones\n",
    "pipeline_stages = indexers + encoders + [assembler]\n",
    "\n",
    "pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "print(f\"\\nPipeline con {len(pipeline_stages)} stages:\")\n",
    "for i, stage in enumerate(pipeline_stages):\n",
    "    print(f\"  Stage {i+1}: {type(stage).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07289172-6083-4fbf-94c8-9495ca668bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 5: Entrenar el pipeline (fit)\n",
    "# Nota: StringIndexer y OneHotEncoder necesitan \"aprender\" del dataset\n",
    "print(\"\\nEntrenando pipeline...\")\n",
    "pipeline_model = pipeline.fit(df_clean)\n",
    "print(\"Pipeline entrenado exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c3d9d4-5806-4329-9557-72df911474a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# PASO 6: Aplicar transformaciones (transform)\n",
    "df_transformed = pipeline_model.transform(df_clean)\n",
    "\n",
    "print(\"\\nTransformación completada\")\n",
    "print(f\"Columnas después de transformar: {len(df_transformed.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5526dc-b473-4a2b-88fd-212209866193",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.select(\"features_raw\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2fdf0-c68a-4de2-b088-7c7be3830739",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.select(\"features_raw\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd281e-1633-441c-b003-a34c93825f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver dimensión del vector de features\n",
    "sample_features = df_transformed.select(\"features_raw\").first()[0]\n",
    "print(f\"Dimensión del vector de features: {len(sample_features)}\")\n",
    "\n",
    "# %%\n",
    "# Mostrar ejemplo de transformación\n",
    "df_transformed.select(\n",
    "    available_cat[0] if available_cat else \"id\",\n",
    "    available_cat[0] + \"_idx\" if available_cat else \"id\",\n",
    "    available_cat[0] + \"_vec\" if available_cat else \"id\",\n",
    "    \"features_raw\"\n",
    ").show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f585d00b-53bc-488a-898f-486bc66f4ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Guardar pipeline entrenado\n",
    "pipeline_path = \"/opt/spark-data/processed/feature_pipeline\"\n",
    "pipeline_model.save(pipeline_path)\n",
    "print(f\"\\nPipeline guardado en: {pipeline_path}\")\n",
    "\n",
    "# %%\n",
    "# Guardar dataset transformado\n",
    "output_path = \"/opt/spark-data/processed/secop_features.parquet\"\n",
    "df_transformed.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"Dataset transformado guardado en: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff8965-9062-44ea-ba3b-bf52e86aba38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
