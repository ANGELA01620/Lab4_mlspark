# %% [markdown]
# # Notebook 07: RegularizaciÃ³n L1, L2 y ElasticNet
#
# SecciÃ³n 14: PrevenciÃ³n de overfitting con regularizaciÃ³n
#
# Objetivo:
# Comparar Ridge (L2), Lasso (L1) y ElasticNet
#
# Conceptos clave:
# - Ridge (L2): regParam > 0, elasticNetParam = 0
# - Lasso (L1): regParam > 0, elasticNetParam = 1
# - ElasticNet: regParam > 0, elasticNetParam âˆˆ (0,1)

# %%
from pyspark.sql import SparkSession
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.functions import col, log1p
import pandas as pd
import numpy as np
import json

# %%
spark = SparkSession.builder \
    .appName("SECOP_Regularizacion") \
    .master("spark://spark-master:7077") \
    .getOrCreate()

# %%
# Cargar datos
df = spark.read.parquet("/opt/spark-data/processed/secop_ml_ready.parquet")

from pyspark.sql.functions import log1p

df = df.withColumnRenamed("features_pca", "features") \
       .filter(col("label").isNotNull())

df = df.withColumn("label", log1p(col("label")))


train, test = df.randomSplit([0.7, 0.3], seed=42)

print(f"Train: {train.count():,}")
print(f"Test: {test.count():,}")

# ============================================================
# RETO 1: Entender la RegularizaciÃ³n
# ============================================================

# Pregunta conceptual:
# Â¿Por quÃ© necesitamos regularizaciÃ³n?
#
# Escenario:
# RÂ² train = 0.95
# RÂ² test = 0.45
#
# Opciones:
# A) Underfitting
# B) Overfitting
# C) Perfecto
# D) MÃ¡s features
#
# Respuesta:
# B) El modelo estÃ¡ overfitting.
#
# ExplicaciÃ³n:
# El modelo aprende demasiado bien el entrenamiento,
# pero generaliza mal en test.
# La regularizaciÃ³n penaliza coeficientes grandes,
# simplifica el modelo y mejora generalizaciÃ³n.

# ============================================================
# RETO 2: Configurar el Evaluador
# ============================================================

# Pregunta:
# Â¿QuÃ© mÃ©trica usarÃ­as para comparar modelos?
# - RMSE
# - MAE
# - RÂ²
#
# Respuesta:
# Usamos RMSE porque penaliza mÃ¡s los errores grandes
# y es adecuada para regresiÃ³n monetaria.

evaluator = RegressionEvaluator(
    labelCol="label",
    predictionCol="prediction",
    metricName="rmse"
)

# ============================================================
# RETO 3: Experimento de RegularizaciÃ³n
# ============================================================

# ParÃ¡metros sugeridos:
# regParam: [0.0, 0.01, 0.1, 1.0, 10.0]
# elasticNetParam: [0.0, 0.5, 1.0]

# ðŸ”¹ Ampliamos rango para ver efecto real de regularizaciÃ³n
reg_params = [0.0, 0.01, 0.1, 1.0, 10.0]
elastic_params = [0.0, 0.5, 1.0]

print(f"Combinaciones totales: {len(reg_params) * len(elastic_params)}")

resultados = []

for reg in reg_params:
    for elastic in elastic_params:

        lr = LinearRegression(
            featuresCol="features",
            labelCol="label",
            maxIter=200,
            regParam=reg,
            elasticNetParam=elastic
        )

        model = lr.fit(train)

        rmse_train = evaluator.evaluate(model.transform(train))
        rmse_test = evaluator.evaluate(model.transform(test))

        if reg == 0.0:
            reg_type = "Sin regularizaciÃ³n"
        elif elastic == 0.0:
            reg_type = "Ridge (L2)"
        elif elastic == 1.0:
            reg_type = "Lasso (L1)"
        else:
            reg_type = "ElasticNet"

        resultados.append({
            "regParam": reg,
            "elasticNetParam": elastic,
            "tipo": reg_type,
            "rmse_train": rmse_train,
            "rmse_test": rmse_test,
            "gap": rmse_test - rmse_train
        })

        print(f"{reg_type:20s} | Î»={reg:7.2f} | Î±={elastic:.1f} | "
              f"Train: {rmse_train:,.4f} | Test: {rmse_test:,.4f}")

# ============================================================
# RETO 4: Analizar Resultados
# ============================================================

df_resultados = pd.DataFrame(resultados)
df_resultados = df_resultados.sort_values("rmse_test")

print("\nResultados ordenados por RMSE Test:")
print(df_resultados.to_string(index=False))

mejor_modelo = df_resultados.iloc[0]

print("\nMejor modelo encontrado:")
print(mejor_modelo)

# Pregunta:
# Â¿El mejor modelo es siempre el menor RMSE test?
#
# Respuesta:
# No necesariamente.
# TambiÃ©n debemos considerar:
# - Gap train-test
# - Estabilidad
# - Interpretabilidad
# - Complejidad del modelo

# ============================================================
# RETO 5: Comparar Overfitting
# ============================================================

print("\nAnÃ¡lisis de Overfitting:")
for _, row in df_resultados.iterrows():
    print(f"{row['tipo']:20s} | Î»={row['regParam']:7.2f} | "
          f"Gap: {row['gap']:,.4f}")

# Preguntas:
# Si regParam=0.0 tiene train bajo y test alto â†’ Overfitting âœ”
# Si regParam=10.0 tiene ambos altos â†’ Underfitting âœ”
#
# Â¿QuÃ© regularizaciÃ³n reduce mÃ¡s el overfitting?
#
# Respuesta:
# Generalmente Ridge o ElasticNet moderado reducen el gap
# sin incrementar demasiado el error total.
#
# Â¿Hay trade-off?
# SÃ­. MÃ¡s regularizaciÃ³n reduce overfitting,
# pero demasiada produce underfitting.

# ============================================================
# RETO 6: Modelo Final
# ============================================================

best_reg = float(mejor_modelo["regParam"])
best_elastic = float(mejor_modelo["elasticNetParam"])

lr_final = LinearRegression(
    featuresCol="features",
    labelCol="label",
    maxIter=200,
    regParam=best_reg,
    elasticNetParam=best_elastic
)

modelo_final = lr_final.fit(train)

rmse_final = evaluator.evaluate(modelo_final.transform(test))

print(f"\nRMSE final del mejor modelo: {rmse_final:,.4f}")

model_path = "/opt/spark-data/processed/regularized_model"

modelo_final.write().overwrite().save(model_path)

print(f"Modelo guardado en: {model_path}")

# ============================================================
# RETO BONUS: Efecto de Lambda
# ============================================================

print("\nEfecto de Lasso en coeficientes:")

for reg in [0.01, 0.1, 1.0, 10.0]:

    lr_lasso = LinearRegression(
        featuresCol="features",
        labelCol="label",
        maxIter=200,
        regParam=reg,
        elasticNetParam=1.0
    )

    model_lasso = lr_lasso.fit(train)

    coefs = np.array(model_lasso.coefficients)
    zeros = np.sum(np.abs(coefs) < 1e-6)

    rmse = evaluator.evaluate(model_lasso.transform(test))

    print(f"Î»={reg:7.2f} | Coeficientes en 0: {zeros}/{len(coefs)} | RMSE: {rmse:,.4f}")

# Pregunta:
# Â¿Por quÃ© Lasso elimina features y Ridge no?
#
# Respuesta:
# Lasso usa penalizaciÃ³n L1 que permite coeficientes exactamente 0.
# Ridge usa L2 que solo reduce magnitudes pero nunca a 0.

# ============================================================
# Preguntas de ReflexiÃ³n
# ============================================================

# 1. Â¿CuÃ¡ndo usar Ridge vs Lasso vs ElasticNet?
# - Ridge: Muchas variables correlacionadas.
# - Lasso: Cuando quieres selecciÃ³n automÃ¡tica.
# - ElasticNet: Cuando hay alta dimensionalidad y correlaciÃ³n.
#
# 2. Â¿QuÃ© pasa si regParam es demasiado grande?
# - El modelo se vuelve demasiado simple (Underfitting).
#
# 3. Â¿Es posible que sin regularizaciÃ³n sea el mejor?
# - SÃ­, si el dataset es grande y tiene poco ruido.
#
# 4. Â¿CÃ³mo elegir regParam en producciÃ³n?
# - CrossValidation con mÃºltiples folds.
# - GridSearch.
# - Evaluar estabilidad temporal.

# ============================================================
# Guardar resultados
# ============================================================

with open("/opt/spark-data/processed/regularizacion_resultados.json", "w") as f:
    json.dump(resultados, f, indent=2)

print("Resultados guardados.")

# %%
print("\n" + "="*60)
print("RESUMEN REGULARIZACIÃ“N")
print("="*60)
print("âœ” Entendido diferencia entre L1, L2 y ElasticNet")
print("âœ” Experimentado con mÃºltiples combinaciones")
print("âœ” Identificado el mejor modelo")
print("âœ” Analizado overfitting vs underfitting")
print("âœ” Guardado modelo final")
print("="*60)

spark.stop()
